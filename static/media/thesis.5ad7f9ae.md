# Abstract

The main objective of this thesis is to increase trust in Machine Learning models by developing tools capable of explaining their predictions and quantifying the associated uncertainty. The first part of this thesis focuses on local explanation methods. We first highlight the limitations of existing estimators of Shapley Values for tree-based models and the issues related to their use with categorical variables. After proposing solutions to these problems, we demonstrate the unreliability of Shapley Values and the LIME method in providing local explanations. Subsequently, we introduce novel explanation techniques, including importance measures, selection of important variable subsets, local decision rules, counterfactual actions, and rule-based counterfactuals. All the proposed methods are "model-free," meaning they do not require access to the underlying model to make predictions. Furthermore, they do not involve generating new observations, thus avoiding the extrapolation problems inherent in most existing methods that rely on predictions using implausible or impossible observations created by randomly combining variable values from multiple instances. Moreover, the proposed methods stand out from the diverse heuristics found in the literature by offering precise definitions of the involved quantities accompanied by consistency results. In the second part of the thesis, we analyze conformal prediction, which allows for constructing predictive intervals with non-asymptotic coverage guarantees based solely on the assumption of exchangeability. We propose a method to make these intervals more adaptive and ensure coverage rates given a single calibration set.

[PDF](https://theses.hal.science/tel-04485328) - [CODE](https://theses.hal.science/tel-04485328)

